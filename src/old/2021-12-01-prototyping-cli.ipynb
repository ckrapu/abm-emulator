{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3db8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from emulators import SpaceTimeKron\n",
    "\n",
    "def dump_default(obj):\n",
    "    '''\n",
    "    Helps json.dumps figure out what to do with\n",
    "    a Numpy array which is not natively serializable.\n",
    "    '''\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    raise TypeError('Not serializable')\n",
    "\n",
    "def average_point(trace):\n",
    "    '''\n",
    "    Compute posterior mean dict from Arviz InferenceData object.\n",
    "    '''\n",
    "    return {k: trace.posterior[k].mean(axis=(0,1)).to_numpy() for k in trace.posterior.keys()}\n",
    "\n",
    "def at_least_2d(array):\n",
    "    '''\n",
    "    Forces a Numpy array to have at least two dimensions.\n",
    "    '''\n",
    "    if array.ndim == 1:\n",
    "        return array[:, None]\n",
    "    else:\n",
    "        return array\n",
    "    \n",
    "def constrain_unit_cube(array, reduce_axis=0):\n",
    "    '''\n",
    "    Subtract minimum and divide by maximum\n",
    "    to ensure returned values are constrained to the \n",
    "    unit cube.\n",
    "    '''\n",
    "    offset = array.min(axis=reduce_axis)\n",
    "    unit_array = array - offset\n",
    "    scale = unit_array.max(axis=reduce_axis)\n",
    "    return unit_array / scale, offset, scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf8c278",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "input_filepath     = '../data/sir.json'\n",
    "fit_method         = 'mcmc'\n",
    "model_type         = 'time+process'\n",
    "output_filepath    = f'../outputs/sir_{fit_method}_{model_type}.json'\n",
    "\n",
    "def fit_model(input_filepath, output_filepath, model_type, fit_method, \n",
    "              response_transform='plus1log', split_char='+',vi_iter=100_000, mcmc_iter=2):\n",
    "    '''\n",
    "    Utility for determining type of surrogate model to construct, preprocessing input/response data, \n",
    "    and running a parameter estimation algorithm.\n",
    "    '''\n",
    "    \n",
    "    if response_transform == 'plus1log':\n",
    "        response_transform = lambda x: np.log(x + 1)\n",
    "    elif response_transform == 'ihs':\n",
    "        response_transform = lambda x: np.log(x + (x**2+1)**0.5)\n",
    "    elif response_transform == 'none':\n",
    "        response_transform = lambda x: x       \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    with open(input_filepath, 'r') as src:\n",
    "        data = json.load(src)\n",
    "\n",
    "    for k in data.keys():\n",
    "        data[k] = np.asarray(data[k])\n",
    "\n",
    "    coord_keys = model_type.split(split_char)\n",
    "\n",
    "    kron_Xs = [at_least_2d(data[k]) for k in coord_keys]\n",
    "\n",
    "    input_scales  = []\n",
    "    input_offsets = []\n",
    "\n",
    "    # Shift and rescale all inputs to unit cube\n",
    "    for i, arr in enumerate(kron_Xs):\n",
    "        unit_array, offset, scale = constrain_unit_cube(arr)\n",
    "\n",
    "        input_scales  += [np.atleast_1d(scale).tolist()]\n",
    "        input_offsets += [np.atleast_1d(offset).tolist()]\n",
    "\n",
    "    # Select only training input points in process variable\n",
    "    # space\n",
    "    kron_Xs[-1] = kron_Xs[-1][data['train_indices']]\n",
    "\n",
    "    # A power transform or similar preprocessing\n",
    "    # technique may be applied at this stage before\n",
    "    # scaling and subtracting an offset.\n",
    "    response_array = response_transform(data[f'{model_type}_response'])\n",
    "    response_axes  = tuple(np.arange(response_array.ndim))\n",
    "    response_array, offset, scale = constrain_unit_cube(response_array, \n",
    "                                                        reduce_axis=response_axes)\n",
    "    response_array = response_array[...,data['train_indices']]\n",
    "    y = response_array.flatten()\n",
    "\n",
    "    extra_bookkeeping = {\n",
    "        'input_scales'    : input_scales,\n",
    "        'input_offsets'   : input_offsets,\n",
    "        'response_offset' : offset,\n",
    "        'response_scale'  : scale\n",
    "    }\n",
    "        \n",
    "    stpk = SpaceTimeKron()\n",
    "    stpk.fit(kron_Xs, y, fit_method=fit_method, vi_iter=vi_iter, mcmc_iter=mcmc_iter)    \n",
    "    \n",
    "    # Merge dictionaries and cast to list type so that target\n",
    "    # arrays are serializable and human-readable\n",
    "    trace = stpk.trace.to_dict()\n",
    "    \n",
    "    for k, v in extra_bookkeeping.items():\n",
    "        if k in trace.keys():\n",
    "            raise KeyError \n",
    "        trace[k] = v\n",
    "    \n",
    "    with open(output_filepath, 'w') as outfile:\n",
    "        json.dump(trace, outfile, default=dump_default)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c41e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abm-venv",
   "language": "python",
   "name": "abm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
